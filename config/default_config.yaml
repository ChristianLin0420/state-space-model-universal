model:
  type: "s4"  # One of: s4, s5, mamba, mamba2, s4d, transformer
  num_layers: 4  # Number of layers in the model
  d_model: 256
  d_state: 64
  dropout: 0.1
  max_seq_len: 2048  # Maximum sequence length
  max_length: 16384  # Maximum sequence length for kernel pre-computation
  # SSM parameters
  hippo_method: "legS"  # one of: "legT", "lagT", "legS"
  bidirectional: false  # for s4
  dt_min: 0.001  # for s4, s5
  dt_max: 0.1  # for s4, s5
  init_scale: 1.0  # for s5
  # Mamba/Mamba2 parameters
  d_conv: 4  # for mamba, mamba2
  expand_factor: 2  # for mamba, mamba2
  conv_kernel_size: 4  # for mamba, mamba2
  selective_scan: true  # for mamba2
  block_type: "sequential"  # for mamba2: "sequential" or "parallel"
  # Transformer parameters
  nhead: 8  # for transformer
  dim_feedforward: 1024  # for transformer
  activation: "gelu"  # for transformer
  layer_norm_eps: 1e-5  # for transformer
  batch_first: true  # for transformer
  norm_first: true  # for transformer
  mode: "conv"  # "conv" for training, "recurrent" for streaming inference
  hippo_theta: 1.0  # sliding window length (only used for LegT)
  
training:
  batch_size: 32
  learning_rate: 0.001
  max_epochs: 100
  optimizer: "adamw"  # one of: adam, adamw
  weight_decay: 0.01  # L2 regularization
  scheduler:
    type: "cosine"  # one of: cosine, linear, constant
    warmup_steps: 1000
    min_lr: 1.0e-6
  gradient_clip_val: 0.5
  
data:
  dataset: "synthetic"  # Example dataset type
  sequence_length: 1024
  train_split: 0.8
  val_split: 0.1
  num_workers: 4
  
logging:
  log_every_n_steps: 100
  wandb_project: "state_space_models"
  save_dir: "checkpoints" 

evaluation:
  checkpoint_path: "checkpoints/best_model.pt"
  batch_size: 32
  max_samples: null  # Set to int to limit number of evaluation samples
  metrics:
    - mse
    - rmse
    - mae
    - nmse 