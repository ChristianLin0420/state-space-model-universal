model:
  type: "s4"  # One of: s4, s5, mamba, mamba2
  num_layers: 4  # Number of layers in the model
  d_model: 256
  d_state: 64
  dropout: 0.1
  max_seq_len: 2048  # Maximum sequence length
  # SSM parameters
  hippo_method: "legendre"  # or "fourier"
  bidirectional: false  # for s4
  dt_min: 0.001  # for s4, s5
  dt_max: 0.1  # for s4, s5
  init_scale: 1.0  # for s5
  expansion_factor: 2  # for mamba, mamba2
  conv_kernel: 4  # for mamba, mamba2
  selective_scan: true  # for mamba2
  
training:
  batch_size: 32
  learning_rate: 0.001
  max_epochs: 100
  optimizer: "adam"
  scheduler: "cosine"
  warmup_steps: 1000
  gradient_clip_val: 0.5
  
data:
  dataset: "synthetic"  # Example dataset type
  sequence_length: 1024
  train_split: 0.8
  val_split: 0.1
  num_workers: 4
  
logging:
  log_every_n_steps: 100
  wandb_project: "state_space_models"
  save_dir: "checkpoints" 

evaluation:
  checkpoint_path: "checkpoints/best_model.pt"
  batch_size: 32
  max_samples: null  # Set to int to limit number of evaluation samples
  metrics:
    - mse
    - rmse
    - mae
    - nmse 